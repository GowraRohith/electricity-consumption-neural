In this example, neural networks are used to forecast energy consumption of the Dublin City Council Civic Offices using data between April 2011 â€“ February 2013.

The original dataset is available from <a href="https://data.gov.ie/dataset/energy-consumption-gas-and-electricity-civic-offices-2009-2012/resource/6091c604-8c94-4b44-ac52-c1694e83d746">data.gov.ie</a>, and daily data was created by summing up the consumption for each day across the 15 minute intervals provided.
<h2>Introduction to LSTM</h2>
LSTMs (or long-short term memory networks) allow for analysis of <strong>sequential</strong> or ordered data with long-term dependencies present. Traditional neural networks fall short when it comes to this task, and in this regard an LSTM will be used to predict electricity consumption patterns in this instance.

One particular advantage of LSTMs compared to models such as ARIMA, is that the data does not necessarily need to be stationary (constant mean, variance, and autocorrelation), in order for LSTM to analyse the same - even if doing so might result in an increase in performance.
<h2>Autocorrelation Plots, Dickey-Fuller test and Log-Transformation</h2>
In order to determine whether <strong>stationarity</strong> is present in our model:
<ol>
 	<li>Autocorrelation and partial autocorrelation plots are generated</li>
 	<li>A Dickey-Fuller test is conducted</li>
 	<li>The time series is log-transformed and the above two procedures are run once again in order to determine the change (if any) in stationarity</li>
</ol>
Firstly, here is a plot of the time series:

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/kilowatts-consumed-per-day.png"><img class="aligncenter size-full wp-image-8885" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/kilowatts-consumed-per-day.png" alt="lstm kilowatts consumed per day" width="640" height="480"></a>

It is observed that the volatility (or change in consumption from one day to the next) is quite high. In this regard, a logarithmic transformation could be of use in attempting to smooth this data somewhat. Before doing so, the ACF and PACF plots are generated, and a Dickey-Fuller test is conducted.

<strong>Autocorrelation Plot</strong>

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/autocorrelation-without-log.png"><img class="aligncenter size-full wp-image-8886" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/autocorrelation-without-log.png" alt="autocorrelation without log" width="640" height="480"></a>

<strong>Partial Autocorrelation Plot</strong>

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/partial-autocorrelation-function.png"><img class="aligncenter size-full wp-image-8888" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/partial-autocorrelation-function.png" alt="partial autocorrelation function" width="640" height="480"></a>

Both the autocorrelation and partial autocorrelation plots exhibit significant volatility, implying that correlations exist across several intervals in the time series.

When a Dickey-Fuller test is run, the following results are yielded:
<pre>&gt;&gt;&gt; # Dickey-Fuller Test
... result = adfuller(data1)
&gt;&gt;&gt; print('ADF Statistic: %f' % result[0])
ADF Statistic: -2.703927
&gt;&gt;&gt; print('p-value: %f' % result[1])
p-value: 0.073361
&gt;&gt;&gt; print('Critical Values:')
Critical Values:
&gt;&gt;&gt; for key, value in result[4].items():
...     print('\t%s: %.3f' % (key, value))
... 
	1%: -3.440
	5%: -2.866
	10%: -2.569
</pre>
With a p-value above 0.05, the null hypothesis of non-stationarity cannot be rejected.
<pre>&gt;&gt;&gt; std1=np.std(dataset)
&gt;&gt;&gt; mean1=np.mean(dataset)
&gt;&gt;&gt; cv1=std1/mean1 #Coefficient of Variation
&gt;&gt;&gt; std1
954.7248
&gt;&gt;&gt; mean1
4043.4302
&gt;&gt;&gt; cv1
0.23611754
</pre>
The coefficient of variation (or mean divided by standard deviation) is 0.236, demonstrating significant volatility in the series.

Now, the data is transformed into logarithmic format.
<pre>from numpy import log
dataset = log(dataset)
</pre>
While the time series remains volatile, the size of the deviations have decreased slightly when expressed in logarithmic format:

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/kilowatts-consumed-per-day-logarithmic-format-1.png"><img class="aligncenter size-full wp-image-8889" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/kilowatts-consumed-per-day-logarithmic-format-1.png" alt="kilowatts consumed per day logarithmic format" width="640" height="480"></a>

Moreover, the coefficient of variation has decreased significantly to 0.0319, implying that the variability of the trend in relation to the mean is significantly lower than previously.
<pre>&gt;&gt;&gt; std2=np.std(dataset)
&gt;&gt;&gt; mean2=np.mean(dataset)
&gt;&gt;&gt; cv2=std2/mean2 #Coefficient of Variation
&gt;&gt;&gt; std2
0.26462445
&gt;&gt;&gt; mean2
8.272395
&gt;&gt;&gt; cv2
0.031988855
</pre>
Again, ACF and PACF plots are generated on the logarithmic data, and a Dickey-Fuller test is conducted once again.

<strong>Autocorrelation Plot</strong>

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/autocorrelation-with-log.png"><img class="aligncenter size-full wp-image-8890" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/autocorrelation-with-log.png" alt="autocorrelation with log" width="640" height="480"></a>

<strong>Partial Autocorrelation Plot</strong>

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/partial-autocorrelation-function-log.png"><img class="aligncenter size-full wp-image-8891" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/partial-autocorrelation-function-log.png" alt="partial autocorrelation function log" width="640" height="480"></a>

<strong>Dickey-Fuller Test</strong>
<pre>&gt;&gt;&gt; # Dickey-Fuller Test
... result = adfuller(logdataset)
&gt;&gt;&gt; print('ADF Statistic: %f' % result[0])
ADF Statistic: -2.804265
&gt;&gt;&gt; print('p-value: %f' % result[1])
p-value: 0.057667
&gt;&gt;&gt; print('Critical Values:')
Critical Values:
&gt;&gt;&gt; for key, value in result[4].items():
...     print('\t%s: %.3f' % (key, value))
... 
	1%: -3.440
	5%: -2.866
	10%: -2.569
</pre>
The p-value for the Dickey-Fuller test has decreased to 0.0576. While this technically does not enter the 5% level of significance threshold necessary to reject the null hypothesis, the logarithmic time series has shown lower volatility based on the CV metric, and therefore this time series is used for forecasting purposes with LSTM.
<h2>Time Series Analysis with LSTM</h2>
Now, the LSTM model itself is used for forecasting purposes.
<h3>Data Processing</h3>
Firstly, the relevant libraries are imported and data processing is carried out:
<pre>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pandas import read_csv
import math
import pylab
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from pandas import Series
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import os;
path="filepath"
os.chdir(path)
os.getcwd()

# Form dataset matrix
def create_dataset(dataset, previous=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-previous-1):
		a = dataset[i:(i+previous), 0]
		dataX.append(a)
		dataY.append(dataset[i + previous, 0])
	return np.array(dataX), np.array(dataY)

# fix random seed for reproducibility
np.random.seed(7)

# load dataset
dataframe = read_csv('data.csv', usecols=[1], engine='python', skipfooter=3)
dataset = dataframe.values
dataset = dataset.astype('float32')

from numpy import log
dataset = log(dataset)

meankwh=np.mean(dataset)

# normalize dataset with MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

# Training and Test data partition
train_size = int(len(dataset) * 0.8)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]

# reshape into X=t-1 and Y=t
previous = 1
X_train, Y_train = create_dataset(train, previous)
X_test, Y_test = create_dataset(test, previous)

# reshape input to be [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))
</pre>
<h3>LSTM Generation and Predictions</h3>
The model is trained over <strong>100</strong> epochs, and the predictions are generated.
<pre># Generate LSTM network
model = Sequential()
model.add(LSTM(4, input_shape=(1, previous)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)

# Generate predictions
trainpred = model.predict(X_train)
testpred = model.predict(X_test)

# Convert predictions back to normal values
trainpred = scaler.inverse_transform(trainpred)
Y_train = scaler.inverse_transform([Y_train])
testpred = scaler.inverse_transform(testpred)
Y_test = scaler.inverse_transform([Y_test])
predictions = testpred

# calculate RMSE
trainScore = math.sqrt(mean_squared_error(Y_train[0], trainpred[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(Y_test[0], testpred[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

# Train predictions
trainpredPlot = np.empty_like(dataset)
trainpredPlot[:, :] = np.nan
trainpredPlot[previous:len(trainpred)+previous, :] = trainpred

# Test predictions
testpredPlot = np.empty_like(dataset)
testpredPlot[:, :] = np.nan
testpredPlot[len(trainpred)+(previous*2)+1:len(dataset)-1, :] = testpred

# Plot all predictions
inversetransform, =plt.plot(scaler.inverse_transform(dataset))
trainpred, =plt.plot(trainpredPlot)
testpred, =plt.plot(testpredPlot)
plt.title("Predicted vs. Actual Consumption")
plt.show()
</pre>
<h3>Accuracy</h3>
Here is the output when 100 epochs are generated:
<pre>Epoch 94/100
 - 2s - loss: 0.0406
Epoch 95/100
 - 2s - loss: 0.0406
Epoch 96/100
 - 2s - loss: 0.0404
Epoch 97/100
 - 2s - loss: 0.0406
Epoch 98/100
 - 2s - loss: 0.0406
Epoch 99/100
 - 2s - loss: 0.0403
Epoch 100/100
 - 2s - loss: 0.0406

&gt;&gt;&gt; # Generate predictions
... trainpred = model.predict(X_train)
&gt;&gt;&gt; testpred = model.predict(X_test)
&gt;&gt;&gt; 
&gt;&gt;&gt; # Convert predictions back to normal values
... trainpred = scaler.inverse_transform(trainpred)
&gt;&gt;&gt; Y_train = scaler.inverse_transform([Y_train])
&gt;&gt;&gt; testpred = scaler.inverse_transform(testpred)
&gt;&gt;&gt; Y_test = scaler.inverse_transform([Y_test])
&gt;&gt;&gt; 
&gt;&gt;&gt; # calculate RMSE
... trainScore = math.sqrt(mean_squared_error(Y_train[0], trainpred[:,0]))
&gt;&gt;&gt; print('Train Score: %.2f RMSE' % (trainScore))
Train Score: 0.24 RMSE
&gt;&gt;&gt; testScore = math.sqrt(mean_squared_error(Y_test[0], testpred[:,0]))
&gt;&gt;&gt; print('Test Score: %.2f RMSE' % (testScore))
Test Score: 0.23 RMSE
</pre>
The model shows a root mean squared error of <strong>0.24</strong> on the training dataset, and <strong>0.23</strong> on the test dataset. The mean kilowatt consumption (expressed in logarithmic format) is <strong>8.27</strong>, which means that the error of 0.23 represents less than 3% of the mean consumption.

Here is the plot of predicted versus actual consumption:

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/predicted-vs-actual-consumption-1-day.png"><img class="aligncenter size-full wp-image-8892" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/predicted-vs-actual-consumption-1-day.png" alt="predicted vs actual consumption 1 day" width="640" height="480"></a>

Interestingly, when the predictions are generated on the raw data (those not converted into logarithmic format), the following training and test errors are yielded:
<pre>&gt;&gt;&gt; # calculate RMSE
... trainScore = math.sqrt(mean_squared_error(Y_train[0], trainpred[:,0]))
&gt;&gt;&gt; print('Train Score: %.2f RMSE' % (trainScore))
Train Score: 840.95 RMSE
&gt;&gt;&gt; testScore = math.sqrt(mean_squared_error(Y_test[0], testpred[:,0]))
&gt;&gt;&gt; print('Test Score: %.2f RMSE' % (testScore))
Test Score: 802.62 RMSE
</pre>
In the context of a mean consumption of 4043 kilowatts per day, the mean squared error for the test score represents nearly 20% of the total mean daily consumption, and is quite high in comparison to that generated on the logarithmic data. Therefore, converting the data into logarithmic format has helped smooth the volatility, and appears to have yielded a lower mean squared error as judged by the test score.

That said, it is important to bear in mind that the prediction was made using 1-day of previous data, i.e. Y represents consumption at time t, while X represents consumption at time t-1, as set by the <strong>previous</strong> variable in the code previously. Let's see what happens if this is increased to <strong>10</strong> and <strong>50</strong> days.

<strong>10 days</strong>

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/over-10-days.png"><img class="aligncenter size-full wp-image-8884" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/over-10-days.png" alt="10 days" width="640" height="480"></a>
<pre>&gt;&gt;&gt; # calculate RMSE
... trainScore = math.sqrt(mean_squared_error(Y_train[0], trainpred[:,0]))
&gt;&gt;&gt; print('Train Score: %.2f RMSE' % (trainScore))
Train Score: 0.08 RMSE
&gt;&gt;&gt; testScore = math.sqrt(mean_squared_error(Y_test[0], testpred[:,0]))
&gt;&gt;&gt; print('Test Score: %.2f RMSE' % (testScore))
Test Score: 0.10 RMSE
</pre>
<strong>50 days</strong>

<a href="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/over-50-days.png"><img class="aligncenter size-full wp-image-8883" src="http://www.michaeljgrogan.com/wp-content/uploads/2018/12/over-50-days.png" alt="50 days" width="640" height="480"></a>
<pre>&gt;&gt;&gt; print('Train Score: %.2f RMSE' % (trainScore))
Train Score: 0.07 RMSE
&gt;&gt;&gt; testScore = math.sqrt(mean_squared_error(Y_test[0], testpred[:,0]))
&gt;&gt;&gt; print('Test Score: %.2f RMSE' % (testScore))
Test Score: 0.10 RMSE
</pre>
We can see that the test error was significantly lower over the 10 and 50-day periods, and the volatility in consumption was much better captured given that the LSTM model took more historical data into account when forecasting.

Given the data is in logarithmic format, it is now possible to obtain the true values of the predictions by obtaining the exponent of the data.

For instance, the <strong>test data</strong> is reshaped with (1, -1):
<pre>&gt;&gt;&gt; Y_test.reshape(1,-1)
array([[7.7722197, 8.277015 , 8.458941 , 8.455311 , 8.447589 , 8.445035, 
 ......
8.425287 , 8.404881 , 8.457063 , 8.423954 , 7.98714 , 7.9003944,
8.240862 , 8.41654 , 8.423854 , 8.437414 , 8.397851 , 7.9047146]],
dtype=float32)</pre>
Using numpy, the exponent is then calculated:
<pre>&gt;&gt;&gt; Y_test=np.exp(Y_test)
array([[2373.7344],
       [3932.4375],
       [4717.062 ],
......
       [4616.6016],
       [4437.52  ],
       [2710.0288]], dtype=float32)


</pre>
The predictions are also reshaped and converted back to the original format through calculating the exponent, and when comparing the predictions to the test data, it was found that:
<ol>
 	<li>The mean percentage error was just over 6%.</li>
 	<li>85% of all predictions showed a deviation of under 10% relative to the actual consumption figures.</li>
</ol>
<h1>Conclusion</h1>
For this example, LSTM proved to be quite accurate at predicting fluctuations in electricity consumption. Moreover, expressing the time series in logarithmic format allowed for a smoothing of the volatility in the data and improved the prediction accuracy of the LSTM.
